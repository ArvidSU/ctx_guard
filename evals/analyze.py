#!/usr/bin/env python3
"""
Analysis tool for aggregating and reporting evaluation results.
"""

import csv
import json
from pathlib import Path
from typing import Dict, List, Optional
import sys
from collections import defaultdict

def load_speed_results(results_file: Path) -> List[Dict]:
    """Load speed evaluation results from CSV."""
    results = []
    with open(results_file, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            results.append({
                'model': row['model'],
                'config_file': row['config_file'],
                'size_factor': float(row['size_factor']),
                'execution_time': float(row['execution_time']),
                'summary_length': int(row['summary_length']),
                'exit_code': int(row['exit_code'])
            })
    return results

def load_quality_results(results_file: Path) -> List[Dict]:
    """Load quality evaluation results from CSV."""
    results = []
    with open(results_file, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            results.append({
                'model': row['model'],
                'config_file': row['config_file'],
                'challenge': row['challenge'],
                'can_solve': row['can_solve'].lower() == 'true',
                'needs_full_output': row['needs_full_output'].lower() == 'true',
                'quality_score': float(row['quality_score']),
                'exit_code': int(row['exit_code'])
            })
    return results

def analyze_speed_results(results: List[Dict]) -> Dict:
    """Analyze speed results and compute statistics."""
    if not results:
        return {}
    
    # Group by model and config
    by_model_config = defaultdict(list)
    for result in results:
        key = (result['model'], result['config_file'])
        by_model_config[key].append(result)
    
    analysis = {}
    for (model, config_file), model_results in by_model_config.items():
        execution_times = [r['execution_time'] for r in model_results]
        summary_lengths = [r['summary_length'] for r in model_results]
        
        analysis[f"{model}::{config_file}"] = {
            'avg_execution_time': sum(execution_times) / len(execution_times),
            'min_execution_time': min(execution_times),
            'max_execution_time': max(execution_times),
            'avg_summary_length': sum(summary_lengths) / len(summary_lengths),
            'total_runs': len(model_results)
        }
    
    return analysis

def analyze_quality_results(results: List[Dict]) -> Dict:
    """Analyze quality results and compute statistics."""
    if not results:
        return {}
    
    # Group by model and config
    by_model_config = defaultdict(list)
    for result in results:
        key = (result['model'], result['config_file'])
        by_model_config[key].append(result)
    
    analysis = {}
    for (model, config_file), model_results in by_model_config.items():
        quality_scores = [r['quality_score'] for r in model_results]
        can_solve_count = sum(1 for r in model_results if r['can_solve'])
        needs_full_output_count = sum(1 for r in model_results if r['needs_full_output'])
        
        analysis[f"{model}::{config_file}"] = {
            'avg_quality_score': sum(quality_scores) / len(quality_scores),
            'min_quality_score': min(quality_scores),
            'max_quality_score': max(quality_scores),
            'solve_rate': can_solve_count / len(model_results),
            'full_output_rate': needs_full_output_count / len(model_results),
            'total_challenges': len(model_results)
        }
    
    return analysis

def generate_markdown_report(speed_analysis: Dict, quality_analysis: Dict, 
                            output_path: Path) -> None:
    """Generate a markdown report from analysis results."""
    with open(output_path, 'w') as f:
        f.write("# Evaluation Results Report\n\n")
        f.write("Generated by ctx_guard evaluation system.\n\n")
        
        if speed_analysis:
            f.write("## Speed Evaluation Results\n\n")
            f.write("| Model | Config | Avg Time (s) | Min Time (s) | Max Time (s) | Avg Summary Length | Runs |\n")
            f.write("|-------|--------|---------------|--------------|--------------|---------------------|------|\n")
            
            for key, stats in sorted(speed_analysis.items()):
                model, config = key.split("::")
                f.write(f"| {model} | {config} | {stats['avg_execution_time']:.3f} | "
                       f"{stats['min_execution_time']:.3f} | {stats['max_execution_time']:.3f} | "
                       f"{stats['avg_summary_length']:.1f} | {stats['total_runs']} |\n")
            f.write("\n")
        
        if quality_analysis:
            f.write("## Quality Evaluation Results\n\n")
            f.write("| Model | Config | Avg Quality | Min Quality | Max Quality | Solve Rate | Full Output Rate | Challenges |\n")
            f.write("|-------|--------|-------------|-------------|-------------|------------|------------------|------------|\n")
            
            for key, stats in sorted(quality_analysis.items()):
                model, config = key.split("::")
                f.write(f"| {model} | {config} | {stats['avg_quality_score']:.3f} | "
                       f"{stats['min_quality_score']:.3f} | {stats['max_quality_score']:.3f} | "
                       f"{stats['solve_rate']:.2%} | {stats['full_output_rate']:.2%} | "
                       f"{stats['total_challenges']} |\n")
            f.write("\n")

def generate_json_report(speed_analysis: Dict, quality_analysis: Dict,
                        output_path: Path) -> None:
    """Generate a JSON report from analysis results."""
    report = {
        'speed': speed_analysis,
        'quality': quality_analysis
    }
    
    with open(output_path, 'w') as f:
        json.dump(report, f, indent=2)

def main():
    """Main entry point."""
    evals_dir = Path(__file__).parent
    results_dir = evals_dir / "results"
    
    if len(sys.argv) < 2:
        print("Usage:")
        print("  python analyze.py <speed_results.csv> [quality_results.csv] [--output report.md]")
        print("  python analyze.py --all [--output report.md]")
        sys.exit(1)
    
    speed_results_file = None
    quality_results_file = None
    output_file = None
    use_all = False
    
    i = 1
    while i < len(sys.argv):
        arg = sys.argv[i]
        if arg == "--all":
            use_all = True
            i += 1
        elif arg == "--output":
            if i + 1 < len(sys.argv):
                output_file = Path(sys.argv[i + 1])
                i += 2
            else:
                print("Error: --output requires a file path")
                sys.exit(1)
        elif not speed_results_file:
            speed_results_file = Path(arg)
            i += 1
        elif not quality_results_file:
            quality_results_file = Path(arg)
            i += 1
        else:
            print(f"Error: Unexpected argument: {arg}")
            sys.exit(1)
    
    if use_all:
        # Find most recent result files
        speed_files = sorted(results_dir.glob("speed_*.csv"), reverse=True)
        quality_files = sorted(results_dir.glob("quality_*.csv"), reverse=True)
        
        if speed_files:
            speed_results_file = speed_files[0]
            print(f"Using speed results: {speed_results_file}")
        if quality_files:
            quality_results_file = quality_files[0]
            print(f"Using quality results: {quality_results_file}")
    
    speed_analysis = {}
    quality_analysis = {}
    
    if speed_results_file and speed_results_file.exists():
        print(f"Loading speed results from {speed_results_file}...")
        speed_results = load_speed_results(speed_results_file)
        speed_analysis = analyze_speed_results(speed_results)
        print(f"Analyzed {len(speed_results)} speed results")
    elif speed_results_file:
        print(f"Warning: Speed results file not found: {speed_results_file}")
    
    if quality_results_file and quality_results_file.exists():
        print(f"Loading quality results from {quality_results_file}...")
        quality_results = load_quality_results(quality_results_file)
        quality_analysis = analyze_quality_results(quality_results)
        print(f"Analyzed {len(quality_results)} quality results")
    elif quality_results_file:
        print(f"Warning: Quality results file not found: {quality_results_file}")
    
    if not speed_analysis and not quality_analysis:
        print("Error: No results to analyze")
        sys.exit(1)
    
    # Generate report
    if output_file:
        if output_file.suffix == '.json':
            generate_json_report(speed_analysis, quality_analysis, output_file)
        else:
            generate_markdown_report(speed_analysis, quality_analysis, output_file)
        print(f"Report written to: {output_file}")
    else:
        # Print summary to console
        print("\n=== Speed Analysis ===")
        for key, stats in sorted(speed_analysis.items()):
            print(f"{key}:")
            print(f"  Avg execution time: {stats['avg_execution_time']:.3f}s")
            print(f"  Avg summary length: {stats['avg_summary_length']:.1f} chars")
        
        print("\n=== Quality Analysis ===")
        for key, stats in sorted(quality_analysis.items()):
            print(f"{key}:")
            print(f"  Avg quality score: {stats['avg_quality_score']:.3f}")
            print(f"  Solve rate: {stats['solve_rate']:.2%}")
            print(f"  Full output needed: {stats['full_output_rate']:.2%}")

if __name__ == "__main__":
    main()


